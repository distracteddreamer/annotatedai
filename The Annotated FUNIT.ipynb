{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# [] Add figures\n",
    "# [] Figure out what happens to running mean and var during inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "The few-shot image translator consists of three sub-networks: a content encoder, a class encoder, and a decoder as visualized in Figure 6.\n",
    "\n",
    "> The generator is defined in the module `FewShotGen` shown in full below. We will subsequently consider how the constituent networks are defined.\n",
    "\n",
    "[TODO: break down the function to produce a high-level overview]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotGen(nn.Module):\n",
    "    def __init__(self, hp):\n",
    "        super(FewShotGen, self).__init__()\n",
    "        nf = hp['nf']\n",
    "        nf_mlp = hp['nf_mlp']\n",
    "        down_class = hp['n_downs_class']\n",
    "        down_content = hp['n_downs_content']\n",
    "        n_mlp_blks = hp['n_mlp_blks']\n",
    "        n_res_blks = hp['n_res_blks']\n",
    "        latent_dim = hp['latent_dim']\n",
    "        self.enc_class_model = ClassModelEncoder(down_class,\n",
    "                                                 3,\n",
    "                                                 nf,\n",
    "                                                 latent_dim,\n",
    "                                                 norm='none',\n",
    "                                                 activ='relu',\n",
    "                                                 pad_type='reflect')\n",
    "\n",
    "        self.enc_content = ContentEncoder(down_content,\n",
    "                                          n_res_blks,\n",
    "                                          3,\n",
    "                                          nf,\n",
    "                                          'in',\n",
    "                                          activ='relu',\n",
    "                                          pad_type='reflect')\n",
    "\n",
    "        self.dec = Decoder(down_content,\n",
    "                           n_res_blks,\n",
    "                           self.enc_content.output_dim,\n",
    "                           3,\n",
    "                           res_norm='adain',\n",
    "                           activ='relu',\n",
    "                           pad_type='reflect')\n",
    "\n",
    "        self.mlp = MLP(latent_dim,\n",
    "                       get_num_adain_params(self.dec),\n",
    "                       nf_mlp,\n",
    "                       n_mlp_blks,\n",
    "                       norm='none',\n",
    "                       activ='relu')\n",
    "\n",
    "    def forward(self, one_image, model_set):\n",
    "        # reconstruct an image\n",
    "        content, model_codes = self.encode(one_image, model_set)\n",
    "        model_code = torch.mean(model_codes, dim=0).unsqueeze(0)\n",
    "        images_trans = self.decode(content, model_code)\n",
    "        return images_trans\n",
    "\n",
    "    def encode(self, one_image, model_set):\n",
    "        # extract content code from the input image\n",
    "        content = self.enc_content(one_image)\n",
    "        # extract model code from the images in the model set\n",
    "        class_codes = self.enc_class_model(model_set)\n",
    "        class_code = torch.mean(class_codes, dim=0).unsqueeze(0)\n",
    "        return content, class_code\n",
    "\n",
    "    def decode(self, content, model_code):\n",
    "        # decode content and style codes to an image\n",
    "        adain_params = self.mlp(model_code)\n",
    "        assign_adain_params(adain_params, self.dec)\n",
    "        images = self.dec(content)\n",
    "        return images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Encoder\n",
    "The content encoder maps the input content image to a content latent code, which is a feature map. If the resolution of the input image is 128x128, the resolution of the feature map will be 16x16 since there are 3 stride-2 down-sampling operations. This feature map is designed to encode class-invariant content information. It should encode locations of the parts but not their class- specific appearances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentEncoder(nn.Module):\n",
    "    def __init__(self, downs, n_res, input_dim, dim, norm, activ, pad_type):\n",
    "        super(ContentEncoder, self).__init__()\n",
    "        self.model = []\n",
    "        self.model += [Conv2dBlock(input_dim, dim, 7, 1, 3,\n",
    "                                   norm=norm,\n",
    "                                   activation=activ,\n",
    "                                   pad_type=pad_type)]\n",
    "        for i in range(downs):\n",
    "            self.model += [Conv2dBlock(dim, 2 * dim, 4, 2, 1,\n",
    "                                       norm=norm,\n",
    "                                       activation=activ,\n",
    "                                       pad_type=pad_type)]\n",
    "            dim *= 2\n",
    "        self.model += [ResBlocks(n_res, dim,\n",
    "                                 norm=norm,\n",
    "                                 activation=activ,\n",
    "                                 pad_type=pad_type)]\n",
    "        self.model = nn.Sequential(*self.model)\n",
    "        self.output_dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Encoder\n",
    "On the other hand, the class encoder maps a set of K class images to a class latent code, which is a vector and is aimed to be class-specific. It first maps each input class image to an intermediate latent code using a VGG-like network. These latent vectors are then element- wise averaged to produce the final class latent code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassModelEncoder(nn.Module):\n",
    "    def __init__(self, downs, ind_im, dim, latent_dim, norm, activ, pad_type):\n",
    "        super(ClassModelEncoder, self).__init__()\n",
    "        self.model = []\n",
    "        self.model += [Conv2dBlock(ind_im, dim, 7, 1, 3,\n",
    "                                   norm=norm,\n",
    "                                   activation=activ,\n",
    "                                   pad_type=pad_type)]\n",
    "        for i in range(2):\n",
    "            self.model += [Conv2dBlock(dim, 2 * dim, 4, 2, 1,\n",
    "                                       norm=norm,\n",
    "                                       activation=activ,\n",
    "                                       pad_type=pad_type)]\n",
    "            dim *= 2\n",
    "        for i in range(downs - 2):\n",
    "            self.model += [Conv2dBlock(dim, dim, 4, 2, 1,\n",
    "                                       norm=norm,\n",
    "                                       activation=activ,\n",
    "                                       pad_type=pad_type)]\n",
    "        self.model += [nn.AdaptiveAvgPool2d(1)]\n",
    "        self.model += [nn.Conv2d(dim, latent_dim, 1, 1, 0)]\n",
    "        self.model = nn.Sequential(*self.model)\n",
    "        self.output_dim = dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "The decoder consists of several adaptive instance normalization (AdaIN) residual blocks [19] followed by a couple of upscale convolutional layers (p3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, ups, n_res, dim, out_dim, res_norm, activ, pad_type):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.model = []\n",
    "        self.model += [ResBlocks(n_res, dim, res_norm,\n",
    "                                 activ, pad_type=pad_type)]\n",
    "        for i in range(ups):\n",
    "            self.model += [nn.Upsample(scale_factor=2),\n",
    "                           Conv2dBlock(dim, dim // 2, 5, 1, 2,\n",
    "                                       norm='in',\n",
    "                                       activation=activ,\n",
    "                                       pad_type=pad_type)]\n",
    "            dim //= 2\n",
    "        self.model += [Conv2dBlock(dim, out_dim, 7, 1, 3,\n",
    "                                   norm='none',\n",
    "                                   activation='tanh',\n",
    "                                   pad_type=pad_type)]\n",
    "        self.model = nn.Sequential(*self.model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> One of the building blocks is a `Conv2d` block which consists of a convolution layer, a normalisation layer and an activation layer with the option of apply the normalisation before or after the activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, ks, st, padding=0,\n",
    "                 norm='none', activation='relu', pad_type='zero',\n",
    "                 use_bias=True, activation_first=False):\n",
    "        super(Conv2dBlock, self).__init__()\n",
    "        self.use_bias = use_bias\n",
    "        self.activation_first = activation_first\n",
    "        # initialize padding\n",
    "        if pad_type == 'reflect':\n",
    "            self.pad = nn.ReflectionPad2d(padding)\n",
    "        elif pad_type == 'replicate':\n",
    "            self.pad = nn.ReplicationPad2d(padding)\n",
    "        elif pad_type == 'zero':\n",
    "            self.pad = nn.ZeroPad2d(padding)\n",
    "        else:\n",
    "            assert 0, \"Unsupported padding type: {}\".format(pad_type)\n",
    "\n",
    "        # initialize normalization\n",
    "        norm_dim = out_dim\n",
    "        if norm == 'bn':\n",
    "            self.norm = nn.BatchNorm2d(norm_dim)\n",
    "        elif norm == 'in':\n",
    "            self.norm = nn.InstanceNorm2d(norm_dim)\n",
    "        elif norm == 'adain':\n",
    "            self.norm = AdaptiveInstanceNorm2d(norm_dim)\n",
    "        elif norm == 'none':\n",
    "            self.norm = None\n",
    "        else:\n",
    "            assert 0, \"Unsupported normalization: {}\".format(norm)\n",
    "\n",
    "        # initialize activation\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU(inplace=True)\n",
    "        elif activation == 'lrelu':\n",
    "            self.activation = nn.LeakyReLU(0.2, inplace=True)\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'none':\n",
    "            self.activation = None\n",
    "        else:\n",
    "            assert 0, \"Unsupported activation: {}\".format(activation)\n",
    "\n",
    "        self.conv = nn.Conv2d(in_dim, out_dim, ks, st, bias=self.use_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.activation_first:\n",
    "            if self.activation:\n",
    "                x = self.activation(x)\n",
    "            x = self.conv(self.pad(x))\n",
    "            if self.norm:\n",
    "                x = self.norm(x)\n",
    "        else:\n",
    "            x = self.conv(self.pad(x))\n",
    "            if self.norm:\n",
    "                x = self.norm(x)\n",
    "            if self.activation:\n",
    "                x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Block configurations\n",
    "\n",
    "> The block configurations are given to the subnetwork when they are initialised in `FewShotGen`. Note I have explicitly added keyword arguments below. [TODO: add these]\n",
    "\n",
    "For the content encoder, each layer is followed by the instance normalization and the ReLU non- linearity. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.enc_content = ContentEncoder(down_content,\n",
    "                                  n_res_blks,\n",
    "                                  3,\n",
    "                                  nf,\n",
    "                                  'in',\n",
    "                                  activ='relu',\n",
    "                                  pad_type='reflect')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the class encoder, each layer is followed by the ReLU nonlinearity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaptive Instance Normalisation\n",
    "\n",
    "As shown in the figure, the decoder first decodes the class-specific latent code to a set of mean and variance vectors ($\\mu_i$, $\\sigma^2_i$) where $i = 1, 2$. These vectors are then used as the affine transformation parameters in the AdaIN residual blocks where $\\sigma^2_i$'s are the scaling factors and $\\mu^2_i$'s are the biases. For each residual block, the same affine transformation is applied to every spatial location in the feature map. It controls how the content latent code are decoded to the output image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The generator is defined in `FewShotGen` which Let us look at the part of the generator that deals with AdaIN. The scale and bias for each for each adaptive instance norm layer are obtained via a single stack of linear layers (the module `MLP` below) whose output is divided into separate components to use as the scale and bias for each AdaIN norm layer. Before calling the decoder each time, the style parameters are assigned to all the AdaIN norm layers in the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotGen(nn.Module):\n",
    "    \n",
    "    # [other functions here]\n",
    "    \n",
    "    def decode(self, content, model_code):\n",
    "        # decode content and style codes to an image\n",
    "        adain_params = self.mlp(model_code)\n",
    "        assign_adain_params(adain_params, self.dec)\n",
    "        images = self.dec(content)\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, dim, n_blk, norm, activ):\n",
    "\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = []\n",
    "        self.model += [LinearBlock(in_dim, dim, norm=norm, activation=activ)]\n",
    "        for i in range(n_blk - 2):\n",
    "            self.model += [LinearBlock(dim, dim, norm=norm, activation=activ)]\n",
    "        self.model += [LinearBlock(dim, out_dim,\n",
    "                                   norm='none', activation='none')]\n",
    "        self.model = nn.Sequential(*self.model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_adain_params(adain_params, model):\n",
    "    # assign the adain_params to the AdaIN layers in model\n",
    "    for m in model.modules():\n",
    "        if m.__class__.__name__ == \"AdaptiveInstanceNorm2d\":\n",
    "            mean = adain_params[:, :m.num_features]\n",
    "            std = adain_params[:, m.num_features:2*m.num_features]\n",
    "            m.bias = mean.contiguous().view(-1)\n",
    "            m.weight = std.contiguous().view(-1)\n",
    "            if adain_params.size(1) > 2*m.num_features:\n",
    "                adain_params = adain_params[:, 2*m.num_features:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AdaIN residual block is a residual block using the AdaIN as the normalization layer. For each sample, AdaIN first normalizes the activations of a sample in each channel to have a zero mean and unit variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that the AdaIN norm layer is implemented as a BatchNorm layer whose $\\gamma$ and $\\beta$ are the style scale and bias respectively. Using a BatchNorm layer means that each time the input will first be normalised a zero mean and unit variance before the style transform is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveInstanceNorm2d(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "        super(AdaptiveInstanceNorm2d, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "        self.register_buffer('running_var', torch.ones(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert self.weight is not None and \\\n",
    "               self.bias is not None, \"Please assign AdaIN weight first\"\n",
    "        b, c = x.size(0), x.size(1)\n",
    "        running_mean = self.running_mean.repeat(b)\n",
    "        running_var = self.running_var.repeat(b)\n",
    "        x_reshaped = x.contiguous().view(1, b * c, *x.size()[2:])\n",
    "        out = F.batch_norm(\n",
    "            x_reshaped, running_mean, running_var, self.weight, self.bias,\n",
    "            True, self.momentum, self.eps)\n",
    "        return out.view(b, c, *x.size()[2:])\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(' + str(self.num_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
